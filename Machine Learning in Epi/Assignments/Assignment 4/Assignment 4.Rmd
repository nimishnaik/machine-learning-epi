---
title: "Assignment 4"
output: word_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Packages Needed for Both Questions

```{r load_packages}
library(tidyverse)
library(janitor)
library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
library(caret)
```

## Part 1: Implementing a Simple Prediction Pipeline

The New York City Department of Health administered a questionnaire on general health and physical activity among residents. Using the dataset class4_p1.csv, fit and evaluate two prediction models using linear regression. The aim of the models is to predict the number of days in a month an individual reported having good physical health (feature name: healthydays). A codebook is provided so you can look-up the meaning and values of each feature in the dataset. (Note the codebook lists information on features that are not included in your dataset).

Your analytic pipeline should include the following:

- Perform basic data cleaning. Note which features are continuous, which are categorical and ensure they are being stored that way in your R dataset (That is, if categorical variables have been read-in as continuous variables, convert them to factors)

- Partition data into training and testing (use a 70/30 split)
 
- Fit two prediction  models using  different subsets of the features in the training data. Features can overlap in the two models, but the feature sets should not be exactly the same across models. Clearly state which features were used in the two models.

- Apply both models within the test data and determine which model is the preferred prediction model using the appropriate evaluation metric(s). 

- Describe one setting (in 1-2 sentences) where the implementation of your final model would be useful.


***

### Step 1: Load and Prepare Dataset

```{r prepdata}
set.seed(123)

#Loading in Data
a4.data<-read.csv("~/Desktop/Nimish/Columbia MPH/4. Spring 2023/Machine Learning in Epi/Assignments/Assignment 4/class4_p1.csv")

#Rename Dataset based on the code book
a4.data <- a4.data %>% 
  dplyr::rename(hypertension = chronic1,
         diabetes = chronic3,
         asthma = chronic4,
         cig_use = tobacco1,
         alc_drink = alcohol1,
         pa_min = gpaq8totmin,
         walk_days = gpaq11days,
         pa_yn = habits5,
         diet = habits7,
         gender = dem3,
         hispanic = dem4,
         us_citizen = dem8)

#Convert all categorical variables to factors   
a4.data <- a4.data %>% 
  mutate(hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         asthma = as.factor(asthma),
         cig_use = as.factor(cig_use),
         alc_drink = as.factor(alc_drink),
         pa_yn = as.factor(pa_yn),
         agegroup = as.factor(agegroup),
         diet = as.factor(diet),
         gender = as.factor(gender),
         hispanic = as.factor(hispanic),
         us_citizen = as.factor(us_citizen),
         povertygroup = as.factor(povertygroup))

#Strip ID
a4.data$X <- NULL

#Remove Missing Variables
a4.data <- na.omit(a4.data)

#Check Data Structure
str(a4.data)

#Finding correlated predictors
a4.data.numeric <- a4.data %>% dplyr::select(where(is.numeric))
correlations<-cor(a4.data.numeric, use="complete.obs")
high.correlations<-findCorrelation(correlations, cutoff=0.4) #None Found

#Centering and Scaling
set.up.preprocess<-preProcess(a4.data, method=c("center", "scale"))

#Output pre-processed values
transformed.vals<-predict(set.up.preprocess, a4.data)
```

### Step 2: Data Partitioning

```{r partitioning}
set.seed(123)

train.index<-createDataPartition(transformed.vals$healthydays, p=0.7, list=FALSE)

a4.data.train<-transformed.vals[train.index,]
a4.data.test<-transformed.vals[-train.index,]


#Construct k-folds in your data
train.folds<-createFolds(transformed.vals$healthydays, k=10, list=FALSE)

```

### Step 3: Training the Model using a Linear Regression Method

```{r training}
set.seed(123)

#Model 1: Comorbidities and Some Modifiable Factors
model1 <- lm(healthydays ~ hypertension + diabetes + bmi + alc_drink + cig_use, data = a4.data.train)

#Model 2: Modifiable Factors and Demographic
model2 <- lm(healthydays ~ cig_use + alc_drink + diet + agegroup + gender + povertygroup + bmi,
             data = a4.data.train)
```

### Step 4: Testing the Model

```{r testing}
set.seed(123)

#Applying the Predictions
prediction1 <- predict(model1, a4.data.test)
prediction2 <- predict(model2, a4.data.test)

```

### Step 5: Model Evaluation

```{r evaluation}
set.seed(123)

#Checking the Mean Square Errors
rmse1 <- RMSE(prediction1, a4.data.test$healthydays)
rmse2 <- RMSE(prediction2, a4.data.test$healthydays)

```

The Root Mean Squared Error for Model 1 is __`r rmse1`__.
The Root Mean Squared Error for Model 2 is __`r rmse2`__.

Since the Mean Squared Error for Model 2 is smaller, the preferred model is __Model 2.__

__Describe one setting (in 1-2 sentences) where the implementation of your final model would be useful.__

This model would be useful in a research setting where health is measured as a function of "healthy days" and we want to study the effect of modifiable factors such as cigarette use, alcohol habits, and diet on overall health. The model would also account for how sociodemographic variables affect the health of patients.


***
## Part 2: Conducting an Unsupervised Analysis

Using the dataset from the Group assignment Part 3 (USArrests), identify clusters using hierarchical analysis. Use an agglomerative algorithm for hierarchical clustering. Use a Euclidian distance measure to construct your dissimilarity matrix.

Conduct a hierarchical clustering analysis. Be sure to specify the linkage method used. Within your analysis, make sure you do both of the following:

- Determine the optimal number of clusters using a clear, data-driven strategy.

- Describe the composition of each cluster in terms of the original input features.
 
Pretend that the data are from 2020 and not 1973. Describe one research question that can be addressed using the newly identified clusters. Briefly comment on any scientific or ethical considerations one should review before using these clusters for your specific question. NOTE: The clusters can be used as an exposure, an outcome or a covariate.


***

### Step 1: Load data and prepare for analysis

```{r dataprep2}
set.seed(123)

#Load in the Data
arrest.data <- USArrests

#Deleting any missing data
arrest.data<-na.omit(arrest.data)

#Check means and SDs to determine if scaling is necessary
colMeans(arrest.data, na.rm=TRUE)
apply(arrest.data, 2, sd, na.rm=TRUE)

#Is scaling necessary? Yes, scaling is necessary.
#Centering and Scaling
set.up.preprocess2 <- preProcess(arrest.data, method=c("center", "scale"))

#Output pre-processed values
arrest.transformed <- predict(set.up.preprocess2, arrest.data)

#Check means and SDs to check if scaling worked
colMeans(arrest.transformed, na.rm=TRUE)
apply(arrest.transformed, 2, sd, na.rm=TRUE)

```

### Step 2: Conduct a hierarchical clustering analysis

```{r optimal_clusters}
set.seed(123)

# Create Dissimilarity matrix
diss.matrix <- dist(arrest.transformed, method = "euclidean")

# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete")

# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)

gap_stat2 <- clusGap(arrest.transformed, FUN = hcut, K.max = 10, B = 50)
gap_stat2
fviz_gap_stat(gap_stat2)

```

__Determine the optimal number of clusters using a clear, data-driven strategy.__

- Using _hierarchical clustering_, we determined that the optimal number of clusters is __3 Clusters.__

``` {r cluster_composition}
set.seed(123)

#Create 3 clusters
clusters.hcut<-hcut(arrest.transformed, k=3, hc_func="hclust", hc_method="complete", hc_metric="euclidian")

#Output size, dendrogram, and  plot of the 3 clusters
clusters.hcut$size
fviz_dend(clusters.hcut, rect=TRUE)
fviz_cluster(clusters.hcut)

input.feature.vals<-cbind(arrest.transformed,cluster=clusters.hcut$cluster)

cluster_info <- input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean)

#Cluster Table
knitr::kable(cluster_info)
```

__Describe the composition of each cluster in terms of the original input features__

Based on a hierarchical clustering analysis, we determined that there are 3 optimal clusters based on the features in the dataset USArrests:

- Cluster 1 is composed of __8 states__ that have __higher__ mean rates of arrest per 10,000 people for murder, assault, and rape than the national average rates, but a __lower__ than national average urban population.

- Cluster 2 is composed of __11 states__ that have __higher__ mean rates of arrest per 10,000 people for murder, assault, and rape than the national average rates, as well as a __higher__ than national average urban population.

- Cluster 3 is composed of __31 states__ that have __lower__ mean rates of arrest per 10,000 people for murder, assault, and rape than the national average rates, as well as a __lower__ than national average urban population.


__Pretend that the data are from 2020 and not 1973. Describe one research question that can be addressed using the newly identified clusters. Briefly comment on any scientific or ethical considerations one should review before using these clusters for your specific question. NOTE: The clusters can be used as an exposure, an outcome or a covariate.__

Using a cluster analysis, we can find unique clusters on different levels and combinations of arrest rates for violent crimes in the United States. By studying the underlying similarities in these clusters, we can answer the research question, __"How do sociodemographic factors such as poverty, education, race and ethnic composition, political composition, and urban composition affect affect the arrest rates for violent crimes in the United States in 2020?"__

Based on previous discussions in this class, we must ensure that we are working on data that is representative, and has been collected without methods that introduce bias. Secondly, clustering methodology should be used to further identify and study the factors that have led to the formation of these clusters. The algorithm should not be misinterpreted and misused to predict states with high arrest rates or to indirectly target policies against vulnerable communities in specific high-crime states.




